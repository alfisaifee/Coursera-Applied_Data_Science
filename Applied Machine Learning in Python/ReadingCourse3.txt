Applied Machine Learning

Week 1 

This Nov. 2016 article by Zachary C. Lipton from the blog Approximately Correct discusses why and how automated processes for decision-making, particularly 
applications of machine learning, can exhibit bias in subtle and not-so-subtle ways. It's self-contained and includes a mini-review of machine learning 
concepts that reinforces what's covered in Module 1. This reading is optional for completion of the course.

http://approximatelycorrect.com/2016/11/07/the-foundations-of-algorithmic-bias/

If you're interested in the more general topic of ethics in data science, we recommend this online course in Data Science Ethics by Prof. H.V. Jagadish 
of the University of Michigan.

https://www.edx.org/course/data-science-ethics-michiganx-ds101x-1



Week 2

This article by Prof. Pedro Domingos provides a bit more background and discussion of the essential concepts in machine learning covered in Modules 1 and 2.
It covers topics such as overfitting, the role of data vs model vs features, and the use of ensembles, where many models are learned instead of just one 
(something we look at with random forests).

Domingos, P. (2012). A few useful things to know about machine learning. Communications of the ACM, 55(10), 78. doi:10.1145/2347736.2347755

This article by Ed Yong in The Scientist is included because it describes a real-world example of a prediction problem in the health/medical sciences 
domain - training a classifier to predict risk of autism spectrum disorder (ASD) based on genetic markers - as well as including discussion of potential 
overfitting of the classifier (by training and testing on the same data) as one possible issue, among other factors, by researchers attempting to replicate 
the study.

http://www.the-scientist.com/?articles.view/articleNo/38030/title/Genetic-Test-for-Autism-Refuted/


Week 3


Beyond the essential evaluation metrics covered in this course, online controlled experiments, which involve A-B testing and other techniques, are perhaps 
the most important way that machine learning algorithms are evaluated for real-world use in Web and other online applications. This article by Ron Kohavi, 
Randal Henne, and Dan Sommerfield, reviews the key points of running controlled experiments, along with important engineering issues and limitations to 
keep in mind when using them.

Kohavi, R., Henne, R. M., & Sommerfield, D. (2007). Practical guide to controlled experiments on the web. Proceedings of the 13th ACM SIGKDD international 
conference on Knowledge discovery and data mining - KDD '07. doi:10.1145/1281192.1281295


Week 4

This tutorial by Ophir Tanz and Cambron Carter is a fun high-level math-free tutorial on neural networks and in particular, goes into more depth on 
convolutional neural networks - a form of neural network with multiple layers of processing that forms the basis for many deep learning systems today 
(see the Deep Learning lecture for more details).

Carter, C., & Tanz, O. (2017, April 13). Neural networks made easy. Retrieved May 10, 2017, from https://techcrunch.com/2017/04/13/neural-networks-made-easy/
-------------------------------------------------------------------------------------------------
This neural network simulation by Daniel Smilkov and Shan Carter lets you play with neural networks in your browser. See the effect of different parameter 
settings and network configurations on a choice of difficult example classification problems.
The "output" on the right shows the "training loss" and "test loss". Loss is an evaluation metric that is related to the number of errors made for each example 
on the training or test set - so lower loss numbers are better. (In technical terms, for neural networks the loss is usually negative log-likelihood for 
classification, and residual sum of squares for regression.)
To show decision boundaries more clearly, along with the test data, click the two checkboxes marked "Show test data" and "Discretize output" in the lower right 
of the window.
To access the simulation, click here:

http://playground.tensorflow.org/
 -------------------------------------------------------------------------------------------------
This self-contained tutorial by Tim Dettmers covers the key high-level concepts of deep learning and reinforces the basic concepts we covered in the Neural 
Networks and Deep Learning lectures. There are multiple parts - Part 1 is less technical while Parts 2-4 go into more detail on algorithms.

The link to access Part 1 is here:

https://devblogs.nvidia.com/parallelforall/deep-learning-nutshell-core-concepts/

Deep Learning in a Nutshell: Core Concepts. (2016, September 08). Retrieved May 10, 2017.
---------------------------------------------------------------------------------------------------
This short article is an example of how deep learning is being used in healthcare.

Assisting Pathologists in Detecting Cancer with Deep Learning

Posted by Martin Stumpe (Technical Lead) and Lily Peng (Product Manager), Google Research Blog
---------------------------------------------------------------------------------------------------
This fun, less-technical read from Colin Fraser reinforces the material in the Data Leakage lecture to provide further explanation and examples on 
detecting and avoiding data leakage in your machine learning applications.

Here's the link to the article:

https://medium.com/@colin.fraser/the-treachery-of-leakage-56a2d7c4e931
---------------------------------------------------------------------------------------------------
If you want an example in more depth of how data scientists are exploring ways to detect and avoid data leakage, this technical article proposes 
one approach: a two-stage process based on "legitimacy tags".

If you're just interested in getting a little more background on the problem along with interesting examples, Sections 1 and 2 (Introduction and Related Work) 
are also useful to read on their own.

Kaufman, S., Rosset, S., & Perlich, C. (2011). Leakage in data mining. Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery 
and data mining - KDD '11. doi:10.1145/2020408.2020496
---------------------------------------------------------------------------------------------------
In 2013 a machine learning competition offered a prize for the most accurate detection of right whale calls based on audio data. The organizers soon discovered 
data leakage problems in the first release of the dataset, and this article explains what happened. It's a short but interesting article that serves as an 
excellent example of how subtle or not-so-subtle leakage can occur in specific features.

https://www.kaggle.com/c/the-icml-2013-whale-challenge-right-whale-redux/discussion/4865#25839#post25839

---------------------------------------------------------------------------------------------------
This optional reading is intended mainly for software engineers who want to build and deploy machine learning applications in production - especially at scale. 
The only background knowledge required are the basic machine learning concepts we've covered so far in this course. Written by Google's Dr. Martin Zinkevich, 
it walks through a set of software engineering best practices for designing and deploying machine learning in software systems - based on years of practical 
experience at Google.

http://martin.zinkevich.org/rules_of_ml/rules_of_ml.pdf

---------------------------------------------------------------------------------------------------
Wattenberg, et al., "How to Use t-SNE Effectively", Distill, 2016. http://doi.org/10.23915/distill.00002

http://distill.pub/2016/misread-tsne/#citation
---------------------------------------------------------------------------------------------------
Gleesen, Peter. "How Machines Make Sense of Big Data: an Introduction to Clustering Algorithms", freeCodeCamp, 2017.

https://medium.freecodecamp.com/how-machines-make-sense-of-big-data-an-introduction-to-clustering-algorithms-4bd97d4fbaba




